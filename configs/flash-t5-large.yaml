model_name: flash-t5-large
version: 3
tokenizer_name: flash-t5-fr-tokenizer
train_dataset: /mnt/data2/culturax_tokenized_nopad_flash-t5/train
valid_dataset: /mnt/data2/culturax_tokenized_nopad_flash-t5/valid
checkpoint_name: false
model_args:
  attention_dropout_rate: 0.0
  dropout_rate: 0.0
  auto_map:
    AutoModel: modeling_flash_t5.FlashT5ForConditionalGeneration
  d_ff: 4096
  d_kv: 64
  d_model: 1024
  decoder_start_token_id: 0
  label_smoothing: 0.1
  max_sequence_length: 1024
  model_type: flash_t5
  num_heads: 16
  num_layers: 24
  position_encoding_type: t5
  relative_attention_max_distance: 128
  relative_attention_num_buckets: 32
  tie_word_embeddings: false
  attention_type: triton
  use_triton_layernorm: false
  use_triton_crossentropy: true
  use_triton_gated_mlp: false
  use_glu_mlp: true
  use_randomized_position_encoding: false
  z_loss: 0.0001
training_args:
  adafactor: false
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-06
  bf16: true
  bf16_full_eval: true
  dataloader_drop_last: true
  dataloader_num_workers: 11
  dataloader_pin_memory: true
  ddp_find_unused_parameters: false
  deepspeed: null
  do_eval: true
  eval_accumulation_steps: 4
  eval_steps: 500
  evaluation_strategy: steps
  gradient_accumulation_steps: 22
  gradient_checkpointing: false
  group_by_length: false
  include_tokens_per_second: true
  learning_rate: 5.0e-3
  logging_steps: 100
  logging_strategy: steps
  lr_scheduler_type: linear
  max_grad_norm: 1.0
  max_steps: 200000
  metric_for_best_model: eval_loss
  num_train_epochs: 3.0
  optim: adamw_torch
  overwrite_output_dir: true
  per_device_eval_batch_size: 24
  per_device_train_batch_size: 96
  remove_unused_columns: false
  resume_from_checkpoint: null
  save_safetensors: true
  save_steps: 2000
  save_strategy: steps
  seed: 42
  torch_compile: true
  warmup_ratio: 0.0
  warmup_steps: 10000
  weight_decay: 0.0
collator_args:
  max_token_length: 512
  max_labels_length: 512
  output_batch_size: 64
